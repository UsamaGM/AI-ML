{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UsamaGM/AI-ML/blob/main/generative_ai_rnn_model_romeo_and_juliet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK4dwUk8C_pT"
      },
      "source": [
        "# Lets make a Generative Model today\n",
        "\n",
        "### Start with importing the necessary librarires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sZNrssCIMeZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc3Kox3jDNqN"
      },
      "source": [
        "### Next, we need to download the script that we are going to train our model on.\n",
        "- In this example, we are going to use Romeo and Juliet by William Shakespeare.\n",
        "\n",
        "### Lets start by downloading the script file from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIV4RMRPIhPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0980ff82-4c92-4c00-ed8d-c95265afea41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6rIZ8d5Dssj"
      },
      "source": [
        "### We can also use any script file from our local storage by using the following method\n",
        "- For this example, we are not going to use this particular method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUdo_gv_JICR"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm1U27_HD86i"
      },
      "source": [
        "### Lets load the script\n",
        "- I am saving the script in a variable\n",
        "- Also, lets see how many characters does our script contain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCwg_OPTIiEO",
        "outputId": "11e508fe-2db4-4c7e-e902-b0c4eea82e05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Meviua8EOkG"
      },
      "source": [
        "### Lets print a random text snippet from the script\n",
        "- Believe me this was not random at all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IEoElczJTk7",
        "outputId": "e5e2e89c-bffc-4ef8-eca0-e07299372806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n"
          ]
        }
      ],
      "source": [
        "print(text[479:998])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsVWF_F4EbYz"
      },
      "source": [
        "# Preprocessing the data\n",
        "\n",
        "### Now that we have the script, we need to convert it into a form that we can feed into our model\n",
        "- First, we are calculating the total number of unique **characters** present in our script and saving it into *vocab* variable\n",
        "- Then, I have defined a method to convert the text in our script into integer array\n",
        "- We will use the method to convert the input text into ints and feed to our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCeHs359KR0b"
      },
      "outputs": [],
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gQ5FGtmF0ya"
      },
      "source": [
        "### Lets use this method to convert our script into integer representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0RzmYunF1gt"
      },
      "outputs": [],
      "source": [
        "text_as_int = text_to_int(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfjl4EMbF861"
      },
      "source": [
        "### Lets see an example of how our text is converted into integer arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eupc2l2DLsqB",
        "outputId": "9759117a-134f-4ca9-f906-3f42ef8613c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text is:  First Citizen\n",
            "Encoded as:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ],
      "source": [
        "print('Text is: ', text[:13])\n",
        "print('Encoded as: ', text_to_int(text[:13]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawd3UJHGEZw"
      },
      "source": [
        "### Now, lets define another method to convert integer back into text\n",
        "- This method will be used to convert the output from our model back into text form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v75y8cICNFw7"
      },
      "outputs": [],
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYnxZjZrGZ6t"
      },
      "source": [
        "# Creating and compiling our model\n",
        "\n",
        "### Lets define the parameters for our model\n",
        "- *seq_length* is the number of characters that a single text sequence will contain when being fed into the model\n",
        "- *examples_per_epoch* is the number of examples/text snippets that will be created after dividing the total text into smaller chunks.\n",
        "  - Notice, that each chunk of the text will have a length of 101 characters as described by *seq_length*\n",
        "  - We will feed 100 characters into the model and it will give us 100 characters in return\n",
        "  - These 100 characters will be 99 characters from our input + 1 that the model predicts should come next\n",
        "  - Example: Input: `Hell` Output: `ello`\n",
        "- Lets finally divide our script into smaller chunks using tensorflow's built in method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Hro5I5ORlO"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qe-Tn8QIHRc"
      },
      "source": [
        "### Next, we will create batches of data\n",
        "- Notice that we are creating batches of 101 characters, this is due to the same reason as described in the previous section\n",
        "- Also notice that we are setting `drop_remainder=True`. This is because we want to trim the text to 101 characters and any remaining characters will not be used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYfg9NJFO1W7"
      },
      "outputs": [],
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y21JppGBI2HY"
      },
      "source": [
        "### Next, we will split the 101 characters into two parts:\n",
        "- The first hundred characters for input text\n",
        "- The last hundred characters for target text (expected output text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcBQfYQBPAqQ"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE9uMIixJUk2"
      },
      "source": [
        "### Lets take a look at 2 examples of input and target text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhkzNLo3P-dE",
        "outputId": "7f3ac36d-aac3-4989-d2fe-be3900622307"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Example\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "TARGET\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "Example\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "TARGET\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ],
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print('\\n\\nExample\\n')\n",
        "  print('INPUT')\n",
        "  print(int_to_text(x))\n",
        "  print('\\nTARGET')\n",
        "  print(int_to_text(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3j4oBQ9JuyG"
      },
      "source": [
        "### Lets finally set the paramters for our model\n",
        "- `BATCH_SIZE` is the number of snippets that will be feed into the model in a single go\n",
        "- `VOCAB_SIZE` is the number of unique characters in our script\n",
        "- `EMBEDDING_DIM` is the number of dimensions that a single character is going to have\n",
        "- `RNN_UNITS` is the\n",
        "- `BUFFER_SIZE` is the number of characters that will be held into buffer at any instance\n",
        "\n",
        "### Lets use these parameters to shuffle our data and create batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9gMcfwwQ3Zm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns_AiefJJlH_"
      },
      "source": [
        "### Building the model\n",
        "- I will use a method to build a model for the given parameters\n",
        "- This is handy as we will create a new model once have trained our current model\n",
        "\n",
        "### I am using the parameters defined above to create the model\n",
        "- Lets take a look at the summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yycumEmWODb",
        "outputId": "4321a9af-b017-4b89-db60-3c2fc98fa73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyilz25yL69A"
      },
      "source": [
        "### Lets use our model without training to make predictions\n",
        "- This will help us understand the output format of our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "766mytiEkPGT",
        "outputId": "cfe0a207-8530-4e2d-9cff-32739a41151d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCvR4-mUMHHc"
      },
      "source": [
        "Lets print the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GjX_--Qku3K",
        "outputId": "fd6381d6-b5c2-4920-8c5f-e0322008d161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 3.77869210e-03 -3.25156888e-03 -1.34758756e-03 ...  1.98414247e-03\n",
            "    5.96947502e-03  5.15735289e-03]\n",
            "  [ 5.47010638e-03 -2.56635691e-03 -1.00021728e-03 ... -9.69405519e-04\n",
            "    9.45107639e-03  3.27426149e-03]\n",
            "  [ 4.80640866e-03  5.12239151e-03 -7.33136851e-03 ...  1.90335559e-04\n",
            "    5.45757217e-03  4.54434566e-03]\n",
            "  ...\n",
            "  [ 8.42446648e-03 -8.75267480e-03  2.89455289e-04 ... -1.85202854e-03\n",
            "    8.89510103e-03  1.10856909e-02]\n",
            "  [ 5.38608246e-03 -1.82073796e-03 -5.50154597e-03 ... -1.68352120e-03\n",
            "    9.06787161e-03  7.35846069e-03]\n",
            "  [ 5.45546552e-03 -1.24247489e-03 -6.97036623e-04 ... -1.15101994e-03\n",
            "    1.85231841e-03  7.19488971e-03]]\n",
            "\n",
            " [[ 1.09037256e-03  9.92627232e-04  1.02701329e-03 ... -2.40163086e-03\n",
            "    6.67476910e-04  3.86962714e-03]\n",
            "  [-5.65607939e-03 -2.72559933e-04  2.09835451e-03 ...  1.98012916e-04\n",
            "    6.29216316e-04  6.29244652e-03]\n",
            "  [-2.48823944e-03  6.81281788e-04  2.68362369e-03 ... -2.36968184e-03\n",
            "    7.44560501e-04  9.22726281e-03]\n",
            "  ...\n",
            "  [ 5.78743592e-03  6.00863248e-03  4.98980796e-03 ... -8.72411672e-03\n",
            "   -1.22668035e-02  1.01944953e-02]\n",
            "  [ 2.23027519e-03  1.03471801e-02 -5.11787832e-04 ... -7.81196635e-03\n",
            "   -1.31577849e-02  5.62174153e-03]\n",
            "  [-1.61875272e-03  6.19815709e-03 -6.06462639e-03 ... -4.53750975e-03\n",
            "   -8.66509881e-03  3.18941544e-03]]\n",
            "\n",
            " [[ 3.77869210e-03 -3.25156888e-03 -1.34758756e-03 ...  1.98414247e-03\n",
            "    5.96947502e-03  5.15735289e-03]\n",
            "  [ 7.49938190e-04  3.04708513e-03 -5.58611937e-03 ...  1.54804939e-03\n",
            "    2.23820494e-03  1.88865419e-03]\n",
            "  [ 3.69690126e-03  5.64524764e-03 -8.61720182e-05 ...  8.29887227e-04\n",
            "   -3.79201811e-04  5.64506697e-03]\n",
            "  ...\n",
            "  [ 3.54933622e-03  7.63112446e-04 -4.23962064e-03 ... -3.26679787e-03\n",
            "    8.64408445e-04  6.20731618e-03]\n",
            "  [ 3.75898089e-04  1.70366326e-03  3.77302570e-03 ... -5.01305377e-03\n",
            "   -2.05530785e-03  4.50599613e-03]\n",
            "  [ 1.61233812e-03 -1.77930971e-03  3.33276275e-03 ... -5.29046264e-03\n",
            "    1.09617412e-03 -1.00569567e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-6.34898664e-04  2.67325691e-03 -2.05618516e-03 ... -4.06613108e-04\n",
            "    8.66809220e-04  3.67143610e-03]\n",
            "  [ 3.30091082e-03 -1.58365793e-03 -3.06041376e-03 ...  2.28284346e-03\n",
            "    6.96400274e-03  8.13274924e-03]\n",
            "  [ 6.25208486e-05 -7.85176875e-04  3.10692634e-03 ... -8.06597527e-05\n",
            "    2.53488659e-03  6.97293691e-03]\n",
            "  ...\n",
            "  [ 2.79316399e-03 -2.40368233e-03  5.97663224e-03 ... -6.00671628e-04\n",
            "    1.26120285e-04  1.12907225e-02]\n",
            "  [ 6.23769965e-03 -7.03409826e-03  3.08597088e-03 ...  1.98010192e-03\n",
            "    6.30725361e-03  1.48782805e-02]\n",
            "  [ 6.68838061e-03 -4.92688688e-03 -4.04411461e-03 ...  3.45730409e-03\n",
            "    2.76228832e-03  1.17384568e-02]]\n",
            "\n",
            " [[ 3.77869210e-03 -3.25156888e-03 -1.34758756e-03 ...  1.98414247e-03\n",
            "    5.96947502e-03  5.15735289e-03]\n",
            "  [ 4.44356818e-04 -1.71620795e-03  4.44184709e-03 ... -8.07845034e-04\n",
            "    1.62101001e-03  4.65598423e-03]\n",
            "  [ 2.33341707e-03 -2.05349550e-03  3.29576083e-03 ... -2.54909857e-03\n",
            "    7.09487312e-03  2.68793385e-03]\n",
            "  ...\n",
            "  [-3.25369043e-03 -2.95903767e-03  1.54286134e-03 ...  3.94808408e-03\n",
            "    3.50545789e-03  1.72182620e-02]\n",
            "  [-8.14510975e-04 -2.76624551e-03  4.70574107e-03 ...  2.65964027e-03\n",
            "   -1.96717586e-03  1.47555592e-02]\n",
            "  [-3.42075969e-03  3.42638395e-03 -1.41344965e-04 ...  1.46303501e-03\n",
            "   -4.73271171e-03  8.96697957e-03]]\n",
            "\n",
            " [[-1.41476805e-04  1.76955329e-03 -5.33990748e-03 ... -1.12526235e-04\n",
            "   -3.26882256e-03  1.17395236e-03]\n",
            "  [ 3.08580277e-03 -2.33823434e-03 -6.03494840e-03 ...  3.53203481e-03\n",
            "    3.82815790e-03  6.89559290e-03]\n",
            "  [ 1.44413672e-04 -6.42010150e-03 -5.47617860e-03 ...  5.39262127e-03\n",
            "    3.86806042e-03  5.88214770e-03]\n",
            "  ...\n",
            "  [ 5.58436662e-03 -1.37319602e-03  5.32655558e-03 ... -3.69735691e-03\n",
            "   -4.91334405e-03  8.75268038e-03]\n",
            "  [ 3.75903165e-03  1.45615486e-03  5.70349395e-03 ... -9.99585958e-04\n",
            "   -4.94517945e-03  2.80599250e-03]\n",
            "  [ 1.85307441e-03 -1.21012027e-03  5.68613783e-03 ...  4.19070339e-03\n",
            "   -9.68143716e-03  6.11713575e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8-fO5aeCdNV"
      },
      "source": [
        "Lets print the first element of the 3 dimensional array and see how it looks like\n",
        "- It contains 100 tensors, the length of sequence, each of size 65, the total number of characters in our vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ3-ApaRk-1d",
        "outputId": "03823a43-ffa0-42d1-d5c8-d910b9bdcbba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00377869 -0.00325157 -0.00134759 ...  0.00198414  0.00596948\n",
            "   0.00515735]\n",
            " [ 0.00547011 -0.00256636 -0.00100022 ... -0.00096941  0.00945108\n",
            "   0.00327426]\n",
            " [ 0.00480641  0.00512239 -0.00733137 ...  0.00019034  0.00545757\n",
            "   0.00454435]\n",
            " ...\n",
            " [ 0.00842447 -0.00875267  0.00028946 ... -0.00185203  0.0088951\n",
            "   0.01108569]\n",
            " [ 0.00538608 -0.00182074 -0.00550155 ... -0.00168352  0.00906787\n",
            "   0.00735846]\n",
            " [ 0.00545547 -0.00124247 -0.00069704 ... -0.00115102  0.00185232\n",
            "   0.00719489]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-hfawClCO1X"
      },
      "source": [
        "Lets print the first sequence of the output (The first element of the array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvMCepTNl8sI",
        "outputId": "6a778a2e-69f0-4c72-fda7-d7ef4fafc7e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 3.7786921e-03 -3.2515689e-03 -1.3475876e-03 -6.4116102e-03\n",
            " -1.3390678e-03 -2.0001805e-03 -1.8254002e-03 -1.1850580e-03\n",
            "  2.8794236e-04  2.6290691e-03 -4.2538685e-03 -1.2449514e-03\n",
            " -2.0209318e-03 -2.7280648e-03  7.6773660e-03  5.3662136e-03\n",
            " -1.6212114e-05  2.6481235e-03  3.0350443e-03  1.3088896e-03\n",
            "  2.8666728e-03 -4.4045663e-03 -3.2891580e-03  2.3083936e-03\n",
            "  3.1935032e-03  2.0685693e-04  2.2611208e-04 -3.3180050e-03\n",
            " -4.5483224e-03 -3.8255309e-03 -7.8752637e-06 -6.1637913e-03\n",
            "  4.6982465e-04 -7.9920818e-04  8.6926785e-04  1.7111460e-03\n",
            "  4.0777801e-03 -1.7489472e-03 -3.0875271e-03 -2.5282311e-03\n",
            " -1.4765607e-04  6.6792029e-03  5.5941348e-03  1.7698761e-03\n",
            " -4.9879057e-03 -8.8077364e-04  2.3747329e-04  1.3650558e-03\n",
            "  3.7870496e-03  2.0191488e-03  8.8639080e-04  3.2697653e-03\n",
            " -2.1529431e-04 -2.5701874e-03  3.6191905e-04 -2.9668782e-04\n",
            " -1.5065344e-03  3.0348403e-04  5.8779465e-03  8.4602442e-03\n",
            "  7.5698420e-03 -8.5325567e-03  1.9841425e-03  5.9694750e-03\n",
            "  5.1573529e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqVkHoAeCYU0"
      },
      "source": [
        "### Now, lets take samples of data\n",
        "- This is just to see all the characters that our model has predicted **without training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOLPVjFxmp2E",
        "outputId": "1320a9fb-7092-4d04-f637-81c769954abb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-tdzWR.QGdZUN!eKcLdN:ZjEiE kVBRvJdg.vF:'Tu\n",
            "sBWRHplO!npi\n",
            "k,?vtHZWmvYWwnDLiWcylEvPD.3TtZfZD$3DI&BJpe\n",
            "?\n"
          ]
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "print(predicted_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCEGvY6vDe41"
      },
      "source": [
        "### Define the loss function\n",
        "- As tensorflow does not have a loss function to deal with 3 dimensional outputs\n",
        "- I have created a simple loss function using tensorflow's `sparse_categorical_crossentropy` loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFrLiP2CpEzC"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08ZMEDcOD1gu"
      },
      "source": [
        "### Next, lets compile our model\n",
        "- I have used `adam` as optimizer function and the loss function defined earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKX3WNeBqMq_"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZV4ew7IECQb"
      },
      "source": [
        "### Creating checkpoints\n",
        "- This is essential as once we have trained our model, we will create a new model and we will need the new model to have the parameters of the old one\n",
        "- This is like writing down notes for future reference, as we humans do it\n",
        "- You will se the use of this later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU2kiomtqXXW"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfI_ORnLElFj"
      },
      "source": [
        "### Now is the time to train our model\n",
        "- I have used 100 epochs as overfitting is not going to be the issue here\n",
        "- Notice that I have used `checkpoint_callback` function to save the checkpoints while training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnE9DuKPq3XO",
        "outputId": "905d6c50-eec4-412b-c0ad-fb76dc9b22ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 15s 65ms/step - loss: 2.5716\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.8590\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.6132\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 12s 63ms/step - loss: 1.4864\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 13s 64ms/step - loss: 1.4105\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 13s 65ms/step - loss: 1.3551\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 13s 65ms/step - loss: 1.3115\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.2730\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.2368\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.2007\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.1654\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 13s 67ms/step - loss: 1.1287\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 15s 68ms/step - loss: 1.0896\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 14s 67ms/step - loss: 1.0494\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.0085\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.9689\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.9268\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8861\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.8471\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.8095\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.7745\n",
            "Epoch 22/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.7410\n",
            "Epoch 23/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.7111\n",
            "Epoch 24/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.6821\n",
            "Epoch 25/100\n",
            "172/172 [==============================] - 14s 68ms/step - loss: 0.6585\n",
            "Epoch 26/100\n",
            "172/172 [==============================] - 15s 69ms/step - loss: 0.6339\n",
            "Epoch 27/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.6141\n",
            "Epoch 28/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5957\n",
            "Epoch 29/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5779\n",
            "Epoch 30/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.5620\n",
            "Epoch 31/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.5482\n",
            "Epoch 32/100\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.5354\n",
            "Epoch 33/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.5244\n",
            "Epoch 34/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.5134\n",
            "Epoch 35/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.5041\n",
            "Epoch 36/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.4949\n",
            "Epoch 37/100\n",
            "172/172 [==============================] - 14s 68ms/step - loss: 0.4863\n",
            "Epoch 38/100\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4820\n",
            "Epoch 39/100\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4739\n",
            "Epoch 40/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4690\n",
            "Epoch 41/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4645\n",
            "Epoch 42/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4579\n",
            "Epoch 43/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4528\n",
            "Epoch 44/100\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4489\n",
            "Epoch 45/100\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.4478\n",
            "Epoch 46/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4446\n",
            "Epoch 47/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.4381\n",
            "Epoch 48/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4369\n",
            "Epoch 49/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.4317\n",
            "Epoch 50/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4306\n",
            "Epoch 51/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4285\n",
            "Epoch 52/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4268\n",
            "Epoch 53/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4247\n",
            "Epoch 54/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4218\n",
            "Epoch 55/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4205\n",
            "Epoch 56/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4170\n",
            "Epoch 57/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4150\n",
            "Epoch 58/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4138\n",
            "Epoch 59/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4147\n",
            "Epoch 60/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.4148\n",
            "Epoch 61/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4157\n",
            "Epoch 62/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4097\n",
            "Epoch 63/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4090\n",
            "Epoch 64/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4084\n",
            "Epoch 65/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4060\n",
            "Epoch 66/100\n",
            "172/172 [==============================] - 15s 69ms/step - loss: 0.4062\n",
            "Epoch 67/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.4071\n",
            "Epoch 68/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4041\n",
            "Epoch 69/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4028\n",
            "Epoch 70/100\n",
            "172/172 [==============================] - 15s 70ms/step - loss: 0.4016\n",
            "Epoch 71/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4002\n",
            "Epoch 72/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4004\n",
            "Epoch 73/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.4000\n",
            "Epoch 74/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.3988\n",
            "Epoch 75/100\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.3978\n",
            "Epoch 76/100\n",
            "172/172 [==============================] - 13s 69ms/step - loss: 0.3986\n",
            "Epoch 77/100\n",
            "172/172 [==============================] - 14s 68ms/step - loss: 0.3989\n",
            "Epoch 78/100\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 0.4000\n",
            "Epoch 79/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.3978\n",
            "Epoch 80/100\n",
            "172/172 [==============================] - 15s 70ms/step - loss: 0.3969\n",
            "Epoch 81/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3962\n",
            "Epoch 82/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3974\n",
            "Epoch 83/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3967\n",
            "Epoch 84/100\n",
            "172/172 [==============================] - 15s 70ms/step - loss: 0.3944\n",
            "Epoch 85/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.3934\n",
            "Epoch 86/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3933\n",
            "Epoch 87/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3948\n",
            "Epoch 88/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.3935\n",
            "Epoch 89/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.3918\n",
            "Epoch 90/100\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 0.3929\n",
            "Epoch 91/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3938\n",
            "Epoch 92/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3928\n",
            "Epoch 93/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3912\n",
            "Epoch 94/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3953\n",
            "Epoch 95/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3947\n",
            "Epoch 96/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3945\n",
            "Epoch 97/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3900\n",
            "Epoch 98/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3912\n",
            "Epoch 99/100\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 0.3873\n",
            "Epoch 100/100\n",
            "172/172 [==============================] - 14s 70ms/step - loss: 0.3912\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(data, epochs=100, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0Upip3FFcs"
      },
      "source": [
        "### Now, lets create a new model with a batch size of 1\n",
        "- This is the model we will be using to make predictions\n",
        "- We are setting batch size as 1 as we will be giving only 1 input text sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT9mIefXrkyc"
      },
      "outputs": [],
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHNInMG-FkOZ"
      },
      "source": [
        "### Load the weights from the previous model\n",
        "- This is the reason of creating checkpoints in the previous part\n",
        "- All the learnings of our model are put into our new model\n",
        "- Notice that I am building the model with tensorshape of 1, None\n",
        "  - This is because we will be providing only 1 text input to the model and it could be of any size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETuMwcCwr3mB"
      },
      "outputs": [],
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VkcpVYFGMyk"
      },
      "source": [
        "### Generate text\n",
        "- I have created a method that receives text, preprocesses it, makes transformations, feeds into the model and return the output of specified length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWVeO-AKsC32"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 0.1\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    predictions = predictions / temperature\n",
        "\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally, it is time to generate some text"
      ],
      "metadata": {
        "id": "t-NeOWLAiNad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3gNunHIu5Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e1b7f9-4301-4679-879d-ed7834e04141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type the starting string: juliet\n",
            "juliet, feed from my soul.\n",
            "Cousin of Buckingham, and you say you, sir,\n",
            "I do repent me; read not a word with you.\n",
            "\n",
            "LUCIO:\n",
            "A little falter, she shall be shriek\n",
            "when you have said 'sh, now 'tis away:\n",
            "If ever sorrower'd like subjects,\n",
            "Yet that, then thus I could formake me faint.\n",
            "Cursue that I should come to me again.\n",
            "\n",
            "YORK:\n",
            "I shuns your father's scept put off.\n",
            "Is't all the world goes have done the thing\n",
            "Unhe creature thee.\n",
            "\n",
            "GRENIO:\n",
            "Good more or some other house of poor selvier.\n",
            "\n",
            "Second Senator:\n",
            "Come, come, he's dead!\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "I never did her and so great a ceremonth as they can go too,\n",
            "Una virginallary, speed up:\n",
            "Ne'er shall make one certainly thou art executed,\n",
            "and the table, now in peace than none.\n",
            "\n",
            "GREMIO:\n",
            "Take he, promise them snot.\n",
            "\n",
            "PAULINA:\n",
            "Not so good, my lord.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "There's some infaction of the deep.\n",
            "\n",
            "LUCENTIO:\n",
            "Here, madam:\n",
            "'Hic ibot, though I the sight and ight\n",
            "acquaint him, where he would not do that ever\n",
            "He heard the name of death in him,\n",
            "If live the air,\n",
            "How he \n"
          ]
        }
      ],
      "source": [
        "inp = input(\"Type the starting string: \")\n",
        "print(generate_text(model, inp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp2 = input(\"Type your starting string: \")\n",
        "print(generate_text(model, inp2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Xh-QIRxvRN",
        "outputId": "c56c791c-8a43-4ec7-c9bd-43fb25794e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type your starting string: juliet\n",
            "juliet,\n",
            "And thou shalt tell the process of their death.\n",
            "Meantime to make his state and part,\n",
            "And I will shake thy nose and my good shift's depart.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Not as thou art, not what's the matter.\n",
            "Provost, how came battled with\n",
            "a sudden cloak! and all this is len to death, her with a fair demand?\n",
            "\n",
            "Provost:\n",
            "Now, my lord, give ules here.\n",
            "\n",
            "GONZALO:\n",
            "No, sir, I say his horse comes, with humily and of preceprehall fly to her blood. Lest thou in about?\n",
            "For what are you, sir? Has he done the time,\n",
            "I with immoditation of bliss and polt men come to her.\n",
            "\n",
            "PETRUCHIO:\n",
            "I swear I'll cuff you, if you hault\n",
            "to the wall: the immedial posence\n",
            "Which now's upon't.\n",
            "\n",
            "AUFIDIUS:\n",
            "Bling me as to speak with him.\n",
            "\n",
            "Pedant:\n",
            "Ay, sir, the fool.\n",
            "We have been supple and living fear: hang from his father?\n",
            "\n",
            "Pedant:\n",
            "Ay, sir, the fool. Was not a mind to hear? I am your either heart.\n",
            "Pardon it, then say so much honour.\n",
            "\n",
            "MENENIUS:\n",
            "There was a time when all the body's members\n",
            "Rebell'd against the belly should be patient.\n",
            "\n",
            "BUC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp3 = input(\"Type your starting string: \")\n",
        "print(generate_text(model, inp3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeAOvlvQyNJC",
        "outputId": "00ae4e28-d920-4971-d576-b82753f653c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type your starting string: juliet\n",
            "juliet, and thy state may do appear,\n",
            "Their needless vouches? Custom calls me to't:\n",
            "What custom wills, in all things should be proud,\n",
            "Here shall not stay to choler them by the worst.\n",
            "\n",
            "GEORGE:\n",
            "What time is more, or not to be found.\n",
            "\n",
            "ROMEO:\n",
            "Here serve to tell your person and my life,\n",
            "And I will make the work about my new give unto you to make me make her stand\n",
            "The first departingham and all the world.\n",
            "\n",
            "GLOUCESTER:\n",
            "The gates made proceed on one another.\n",
            "\n",
            "CURTIS:\n",
            "And were I spill this?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Good to know how shall we hear their witness:\n",
            "If it be so, then to dispatch my hands:\n",
            "I will not farther better with her cheek.\n",
            "\n",
            "BENVOLIO:\n",
            "A cold and other part of bats:\n",
            "If every fair prophecy live\n",
            "To his contentious land.\n",
            "O, which is this true so?\n",
            "\n",
            "MONTAGUE:\n",
            "Then we will prove the forfeits on the Tower,\n",
            "An be as present for a place,\n",
            "Have done the time with purpose and to pierce and one half so gentle by secret honour to the ground,\n",
            "And make him all afactors from the fatal bawd;\n",
            "Ingeach'd all the lords\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO6AwQE0qHAiVpGhS3Ep8yG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}